Workflow for detecting and exploring just outliers
========================================================

```{r, eval=TRUE, echo=FALSE}
knitr::opts_chunk$set(
  fig.width=10,
  fig.path='figure/'
)
```

### Date 

Compiled on `r Sys.time()`

### Setup

> change directory to /data-quality/alerts/

```{r child='alertssetup.Rmd'}
```

```{r child='alertssetup.Rmd', eval=FALSE}
knitr::purl("alertssetup.Rmd")
source("alertssetup.R")
unlink("alertssetup.R")
```

### Get alerts data by alert class

#### By class, change class_name var at top

```{r}
class_name = 'HtmlRatioTooHighError'
```

Get data

```{r}
(res <- alerts_by_class(class_name, limit=2000))
# remove bad data
res <- res %>%
  filter(!is.na(article))
```

Extract top N articles, get DOIs

```{r}
num_get <- 10
toinspect <- res[1:num_get,] %>% select(-class)
(dois <- toinspect$article)
```

Browse to an article

```{r eval=FALSE}
browseURL(sprintf("http://alm.plos.org/articles/info:doi/%s", res$article[2]))
```


Get ALM events data and merge alerts data to it

```{r}
alldf <- add_events_data(toinspect, dois)
```

ggplot elements to reuse

```{r}
gg <- function(){
  list(geom_line(size = 2, alpha = 0.6),
       geom_vline(aes(xintercept=as.numeric(create_date)), linetype="longdash"),
       ggtitle("HtmlRatioTooHighError - Top ten highest HTML/PDF ratio articles\n"),
       facet_wrap(~ article, ncol = 2, scales = "free"),
       labs(y="", x=""),
       theme_grey(base_size = 14))
}
```

The distribution of html/pdf ratios

```{r ratio_dist}
res %>%
  ggplot(aes(x=val)) + geom_histogram()
```


Plot html and pdf views, just top `r num_get`

```{r fig.width=10}
alldf %>%
  select(-year, -month, -id, -val, -source, -xml_views, -ratio) %>%
  gather(metric, value, -article, -date, -create_date) %>% 
  ggplot(aes(date, value, color=metric)) + gg()
```

The HTML/PDF ratio, just top `r num_get`

```{r fig.width=10}
alldf %>%
  select(-year, -month, -id, -val, -source, -xml_views, -html_views, -pdf_views) %>%
  ggplot(aes(date, ratio)) + gg()
```

All ratio lines together

```{r fig.width=10}
(alldf_alldois <- add_events_data(res, res$article))

alldf_alldois %>%
  select(article, date, create_date, ratio) %>%
  ggplot(aes(date, log10(ratio), group=article)) + 
    geom_line() +
    labs(y="", x="") +
    theme_grey(base_size = 14)
```

Dig in to particular DOIs. This is rather free-form, depends on the metric of interest.

```{r eval=FALSE}
doi1 <- '10.1371/journal.pbio.0040066'
alm_events(doi1, source = "facebook")
alm_ids(doi1, info = "detail")
```

Are the high value offender DOIs associated with other altmetrics, like social media metrics

Get data from `alm` R package

```{r}
dat <- alm_ids(res$article)
```

```{r}
datdf <- rbind_all(
  Map(function(x, y) data.frame(article=y, x, stringsAsFactors = FALSE), dat$data, res$article)
)
datdf <- inner_join(datdf, res %>% filter(article %in% res$article) %>% select(article, val))
```

Get html views for each article, join to data

```{r}
htmls <- datdf %>% filter(.id == "counter") %>% select(article, html) %>% rename(html_views=html)
datdf <- inner_join(datdf, htmls)
```

Select a subset of metrics

```{r}
datdf <- datdf %>%
  filter(!.id %in% c('citeulike','copernicus','datacite','openedition',
                     'scienceseeker','counter','reddit'))
```

Plot the data

```{r fig.width=10}
datdf %>% 
  ggplot(aes(x=log10(html_views+1), y=log10(total+1))) + 
    geom_point(aes(size=2)) +
    facet_wrap(~ .id, scales='free') +
    labs(y="", x="") +
    theme_grey(base_size = 18) +
    theme(legend.position="none")
```

Are there combinations of metrics that give stronger prediction of the outlier articles

```{r}
log10_1 <- function(x) log10(x + 1)

analyze <- datdf %>%
  filter(.id %in% c("twitter","facebook","mendeley","wikipedia")) %>%
  select(article, .id, total, html_views) %>%
  spread(.id, total)

analyze$html_views <- log10_1(analyze$html_views)
analyze$facebook <- log10_1(analyze$facebook)
analyze$mendeley <- log10_1(analyze$mendeley)
analyze$twitter <- log10_1(analyze$twitter)
analyze$wikipedia <- log10_1(analyze$wikipedia)
  
analyze %>%
  lm(html_views ~ facebook + mendeley + twitter + wikipedia, data = .) %>%
  summary
```



Detect spikes/patterns in signals through time

```{r, eval=FALSE}
'not done yet...'
```

> based on the above work, identify which articles are deserving of further inspection/flagging - perhaps need to look at log files for IP addresses, etc.
